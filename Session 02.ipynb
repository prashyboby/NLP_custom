{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1 - Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, RegexpStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---going---\n",
      "Regex=going\n",
      "Porter=go\n",
      "Lancaster=going\n",
      "WordNet=go\n",
      "\n",
      "---went---\n",
      "Regex=went\n",
      "Porter=went\n",
      "Lancaster=went\n",
      "WordNet=go\n",
      "\n",
      "---thinking---\n",
      "Regex=inking\n",
      "Porter=think\n",
      "Lancaster=think\n",
      "WordNet=think\n",
      "\n",
      "---visited---\n",
      "Regex=visit\n",
      "Porter=visit\n",
      "Lancaster=visit\n",
      "WordNet=visit\n",
      "\n",
      "---houses---\n",
      "Regex=houses\n",
      "Porter=hous\n",
      "Lancaster=hous\n",
      "WordNet=house\n",
      "\n",
      "---elevators---\n",
      "Regex=elevators\n",
      "Porter=elev\n",
      "Lancaster=elev\n",
      "WordNet=elevators\n",
      "\n",
      "---thought---\n",
      "Regex=ought\n",
      "Porter=thought\n",
      "Lancaster=thought\n",
      "WordNet=think\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = ['going','went','thinking','visited','houses','elevators', 'thought']\n",
    "\n",
    "\n",
    "rgst = RegexpStemmer(r'(^th)|(ed$)')\n",
    "pst = PorterStemmer()\n",
    "lst = LancasterStemmer()\n",
    "wnlt = WordNetLemmatizer()\n",
    "\n",
    "for word in words:\n",
    "    print(\"---\"+word+\"---\")\n",
    "    print(\"Regex=\"+rgst.stem(word))\n",
    "    print(\"Porter=\"+pst.stem(word))\n",
    "    print(\"Lancaster=\"+lst.stem(word)) \n",
    "    print(\"WordNet=\"+wnlt.lemmatize(word, pos='v'))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2 - Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, WordPunctTokenizer, WhitespaceTokenizer, MWETokenizer\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt = WordPunctTokenizer()\n",
    "wst = WhitespaceTokenizer()\n",
    "mwet = MWETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Lorem ---\n",
      "Word Tok. 77 words\n",
      "['Lorem', 'ipsum', 'dolor', 'sit', 'amet', ',', 'consectetur', 'adipiscing', 'elit', ',']\n",
      "\n",
      "W. Punct. Tok. 77 words\n",
      "['Lorem', 'ipsum', 'dolor', 'sit', 'amet', ',', 'consectetur', 'adipiscing', 'elit', ',']\n",
      "\n",
      "Whitespace Tok. 69 words\n",
      "['Lorem', 'ipsum', 'dolor', 'sit', 'amet,', 'consectetur', 'adipiscing', 'elit,', 'sed', 'do']\n",
      "\n",
      "Multiword Tok 69 words\n",
      "['Lorem', 'ipsum', 'dolor', 'sit', 'amet,', 'consectetur', 'adipiscing', 'elit,', 'sed', 'do']\n",
      "\n",
      "---- Mr Robinson ---\n",
      "Word Tok. 30 words\n",
      "['My', 'name', 'is', 'Mr.', 'Robinson', 'and', 'I', 'hold', 'a', 'Msc']\n",
      "\n",
      "W. Punct. Tok. 31 words\n",
      "['My', 'name', 'is', 'Mr', '.', 'Robinson', 'and', 'I', 'hold', 'a']\n",
      "\n",
      "Whitespace Tok. 26 words\n",
      "['My', 'name', 'is', 'Mr.', 'Robinson', 'and', 'I', 'hold', 'a', 'Msc.', 'in', 'Computer', 'Science', 'at', 'Stanford', 'University', 'in', 'USA.', 'Nice', 'to']\n",
      "\n",
      "Multiword Tok 25 words\n",
      "['My', 'name', 'is', 'Mr.', 'Robinson', 'and', 'I', 'hold', 'a', 'Msc.', 'in', 'Computer_Science', 'at', 'Stanford', 'University', 'in', 'USA.', 'Nice', 'to', 'meet']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_words(text, wl=10):\n",
    "    print(\"Word Tok. %d words\" % len(word_tokenize(text)))\n",
    "    print(word_tokenize(text)[:wl])\n",
    "    print(\"\")\n",
    "\n",
    "def print_words_wpt(text, wl=10):\n",
    "    print(\"W. Punct. Tok. %d words\" % len(wpt.tokenize(text)))\n",
    "    print(wpt.tokenize(text)[:wl])\n",
    "    print(\"\")\n",
    "\n",
    "def print_words_wst(text, wl=10):\n",
    "    print(\"Whitespace Tok. %d words\" % len(wst.tokenize(text)))\n",
    "    print(wst.tokenize(text)[:wl])\n",
    "    print(\"\")\n",
    "\n",
    "def print_words_mwet(text, wl=10):\n",
    "    print(\"Multiword Tok %d words\" % len(mwet.tokenize(text.split())))\n",
    "    print(mwet.tokenize(text.split())[:wl])\n",
    "    print(\"\")\n",
    "\n",
    "lorem = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit  esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\n",
    "rob = \"My name is Mr. Robinson and I hold a Msc. in Computer Science at Stanford University in USA. Nice to meet you... what was your name?\"\n",
    "\n",
    "mwet.add_mwe(('Computer', 'Science'))\n",
    "#mwet.add_mwe(('Stanford', 'University'))\n",
    "\n",
    "\n",
    "print(\"---- Lorem ---\")\n",
    "\n",
    "print_words(lorem)\n",
    "print_words_wpt(lorem)\n",
    "print_words_wst(lorem)\n",
    "print_words_mwet(lorem)\n",
    "\n",
    "print(\"---- Mr Robinson ---\")\n",
    "\n",
    "print_words(rob)\n",
    "print_words_wpt(rob)\n",
    "print_words_wst(rob, wl=20)\n",
    "print_words_mwet(rob, wl=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3 - Sentence Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "import nltk.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spanish tokenizer from pickle\n",
    "spanish_tokenizer = nltk.data.load('tokenizers/punkt/spanish.pickle')\n",
    "\n",
    "ppar = PunktParameters()\n",
    "abbs = ['msc']\n",
    "ppar.abbrev_types = set(abbs)\n",
    "pst = PunktSentenceTokenizer(ppar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Sentence Tok. 4 sentences\n",
      "> Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\n",
      "\n",
      "> Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\n",
      "\n",
      "> Duis aute irure dolor in reprehenderit in voluptate velit  esse cillum dolore eu fugiat nulla pariatur.\n",
      "\n",
      "> Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
      "\n",
      "-- Sentence Tok. 3 sentences\n",
      "> My name is Mr. Robinson and I hold a MSc.\n",
      "\n",
      "> in Computer Science at Stanford University in USA.\n",
      "\n",
      "> Nice to meet you... what was your name?\n",
      "\n",
      "-- Punkt Sentence Tok. 3 sentences\n",
      "> My name is Mr.\n",
      "\n",
      "> Robinson and I hold a MSc. in Computer Science at Stanford University in USA.\n",
      "\n",
      "> Nice to meet you... what was your name?\n",
      "\n",
      "-- Sent. Spanish Tok. 3 sentences\n",
      "> ¡Hola a todos, bienvenidos a NLP!\n",
      "\n",
      "> Mi nombre es Pedro.\n",
      "\n",
      "> ¿Que os parece el procesamiento de lenguaje natural?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_sent(text):\n",
    "    print (\"-- Sentence Tok. %d sentences\" % len(sent_tokenize(text)))\n",
    "    for s in sent_tokenize(text):\n",
    "        print(\"> %s\\n\" % s)\n",
    "\n",
    "def print_sent_es(text):\n",
    "    print (\"-- Sent. Spanish Tok. %d sentences\" % len(spanish_tokenizer.tokenize(text)))\n",
    "    for s in spanish_tokenizer.tokenize(text):\n",
    "        print(\"> %s\\n\" % s)\n",
    "\n",
    "\n",
    "def print_sent_pst(text):\n",
    "    print (\"-- Punkt Sentence Tok. %d sentences\" % len(pst.tokenize(text)))\n",
    "    for s in pst.tokenize(text):\n",
    "        print(\"> %s\\n\" % s)\n",
    "\n",
    "\n",
    "lorem = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit  esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\n",
    "rob = \"My name is Mr. Robinson and I hold a MSc. in Computer Science at Stanford University in USA. Nice to meet you... what was your name?\"\n",
    "hola = u\"¡Hola a todos, bienvenidos a NLP! Mi nombre es Pedro. ¿Que os parece el procesamiento de lenguaje natural?\"\n",
    "\n",
    "print_sent(lorem)\n",
    "print_sent(rob)\n",
    "print_sent_pst(rob)\n",
    "print_sent_es(hola)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---going---\n",
      "WordNet=go\n",
      "\n",
      "---went---\n",
      "WordNet=go\n",
      "\n",
      "---thinking---\n",
      "WordNet=think\n",
      "\n",
      "---visited---\n",
      "WordNet=visit\n",
      "\n",
      "---houses---\n",
      "WordNet=house\n",
      "\n",
      "---elevators---\n",
      "WordNet=elevators\n",
      "\n",
      "---thought---\n",
      "WordNet=think\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = ['going','went','thinking','visited','houses','elevators', 'thought']\n",
    "\n",
    "\n",
    "rgst = RegexpStemmer(r'(^th)|(ed$)')\n",
    "pst = PorterStemmer()\n",
    "lst = LancasterStemmer()\n",
    "wnlt = WordNetLemmatizer()\n",
    "\n",
    "for word in words:\n",
    "    print(\"---\"+word+\"---\")\n",
    "#     print(\"Regex=\"+rgst.lemmatize(word))\n",
    "#     print(\"Porter=\"+pst.stem(word))\n",
    "#     print(\"Lancaster=\"+lst.stem(word)) \n",
    "    print(\"WordNet=\"+wnlt.lemmatize(word, pos='v'))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
